{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import configparser\n",
    "import pathlib\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "from langdetect import detect\n",
    "import html2text\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSitesConfigParser(): # naive class    \n",
    "    def __init__(self, file_path):\n",
    "        self._config_file = file_path\n",
    "        self._sections = defaultdict(list)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self._sections[index]\n",
    "    \n",
    "    def read(self):\n",
    "        with open(self._config_file) as cf:\n",
    "            lines = cf.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            if line.startswith(\"[\"):\n",
    "                section_name = line.strip('\\n[]')\n",
    "                continue\n",
    "            self._sections[section_name].append(line.strip('\\n'))\n",
    "        \n",
    "    def get_sections(self):\n",
    "        return list(self._sections.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_content(url): #1\n",
    "    try:\n",
    "        url_res = requests.get(url).text\n",
    "    except requests.TooManyRedirects as tmr:\n",
    "        raise(tmr)\n",
    "    else:\n",
    "        html = url_res\n",
    "        text = html2text.html2text(html)\n",
    "        file = open(\"data/raw/site_{url}.txt\".format(counter), 'w+')    \n",
    "        file.write(text.strip('/n').encode('utf-8'))\n",
    "        file.close()\n",
    "    return text\n",
    "\n",
    "def format_web_content(content): #2\n",
    "    a,b = 'áéíóúü','aeiouu'\n",
    "    trans = str.maketrans(a,b)\n",
    "    stop_words = set(stopwords.words(\"spanish\"))\n",
    "    content = content.translate(trans)    \n",
    "    tokens = re.findall(r\"[a-zA-Z]{3,}\", content)\n",
    "    filtered_sentence = [w.lower() for w in tokens if not w in stop_words]    \n",
    "    spanish = [word for word in filtered_sentence if detect(word) == 'es']\n",
    "    return ' '.join(spanish)\n",
    "\n",
    "def build_data_set(dict_):\n",
    "    if not isinstance(dict_, dict):\n",
    "        type_ = type(dict_)\n",
    "        raise TypeError(\"Dictionary expected, {} found\".format(type_))\n",
    "    df = pd.DataFrame(columns = ['manually_assigned_tags', 'url', 'content'])\n",
    "    for key in dict_.keys():\n",
    "        for url in dict_[key]:\n",
    "            try:\n",
    "                content = format_web_content(get_web_content(url))\n",
    "                entry = {'manually_assigned_tag': key, 'url': url, 'content': content}\n",
    "                df.append(pd.DataFrame(entry))\n",
    "            except requests.TooManyRedirects:\n",
    "                print('Too many redirects exception raised. Ignoring website...')\n",
    "                continue\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
