{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sofia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "originData = pd.read_csv(\"../data/interim/dataset_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>manually_assigned_tag</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>supermarket</td>\n",
       "      <td>https://www.walmart.com.ar</td>\n",
       "      <td>nueva nueva correctamente puede cerrar lacteos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>supermarket</td>\n",
       "      <td>https://www.makro.com.ar/ofertas</td>\n",
       "      <td>logo entra cuenta quiero fornecedor sustentabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>supermarket</td>\n",
       "      <td>https://www.alvearsupermercados.com.ar/ofertas/</td>\n",
       "      <td>alvearsupermercados logo blanco alvearsupermer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>supermarket</td>\n",
       "      <td>https://www.cotodigital3.com.ar/sitios/cdigi/b...</td>\n",
       "      <td>experiencia descuentos descuentos comparacione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>telephone</td>\n",
       "      <td>https://www.personal.com.ar/</td>\n",
       "      <td>micuenta destinos cuenta micuenta cuenta cuota...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 manually_assigned_tag  \\\n",
       "0           0           supermarket   \n",
       "1           0           supermarket   \n",
       "2           0           supermarket   \n",
       "3           0           supermarket   \n",
       "4           0             telephone   \n",
       "\n",
       "                                                 url  \\\n",
       "0                         https://www.walmart.com.ar   \n",
       "1                   https://www.makro.com.ar/ofertas   \n",
       "2    https://www.alvearsupermercados.com.ar/ofertas/   \n",
       "3  https://www.cotodigital3.com.ar/sitios/cdigi/b...   \n",
       "4                       https://www.personal.com.ar/   \n",
       "\n",
       "                                             content  \n",
       "0  nueva nueva correctamente puede cerrar lacteos...  \n",
       "1  logo entra cuenta quiero fornecedor sustentabi...  \n",
       "2  alvearsupermercados logo blanco alvearsupermer...  \n",
       "3  experiencia descuentos descuentos comparacione...  \n",
       "4  micuenta destinos cuenta micuenta cuenta cuota...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "originData = originData.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['manually_assigned_tag', 'url', 'content'], dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manually_assigned_tag</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>supermarket</td>\n",
       "      <td>https://www.walmart.com.ar</td>\n",
       "      <td>nueva nueva correctamente puede cerrar lacteos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>supermarket</td>\n",
       "      <td>https://www.makro.com.ar/ofertas</td>\n",
       "      <td>logo entra cuenta quiero fornecedor sustentabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>supermarket</td>\n",
       "      <td>https://www.alvearsupermercados.com.ar/ofertas/</td>\n",
       "      <td>alvearsupermercados logo blanco alvearsupermer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>supermarket</td>\n",
       "      <td>https://www.cotodigital3.com.ar/sitios/cdigi/b...</td>\n",
       "      <td>experiencia descuentos descuentos comparacione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>https://www.personal.com.ar/</td>\n",
       "      <td>micuenta destinos cuenta micuenta cuenta cuota...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  manually_assigned_tag                                                url  \\\n",
       "0           supermarket                         https://www.walmart.com.ar   \n",
       "1           supermarket                   https://www.makro.com.ar/ofertas   \n",
       "2           supermarket    https://www.alvearsupermercados.com.ar/ofertas/   \n",
       "3           supermarket  https://www.cotodigital3.com.ar/sitios/cdigi/b...   \n",
       "4             telephone                       https://www.personal.com.ar/   \n",
       "\n",
       "                                             content  \n",
       "0  nueva nueva correctamente puede cerrar lacteos...  \n",
       "1  logo entra cuenta quiero fornecedor sustentabi...  \n",
       "2  alvearsupermercados logo blanco alvearsupermer...  \n",
       "3  experiencia descuentos descuentos comparacione...  \n",
       "4  micuenta destinos cuenta micuenta cuenta cuota...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Stemmer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Stemmer.py\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "class Stemmer(): # naive class\n",
    "    def __init__(self):\n",
    "        self.stems = list()\n",
    "        self.stemmed_freqs = defaultdict(list)\n",
    "        self.tokens = list()\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        '''\n",
    "        tokenize document and remove (some of) non-spanish words\n",
    "        '''\n",
    "        if not isinstance(document, list):\n",
    "            if isinstance(document, str):\n",
    "                self.tokens = document.split(' ')\n",
    "            else:\n",
    "                raise TypeError('str or dict expected, {} found.'.format(type(document)))\n",
    "        else:\n",
    "            self.tokens = document\n",
    "    \n",
    "    def stemm(self, document):\n",
    "        '''\n",
    "        document stemming\n",
    "        '''\n",
    "        if len(self.tokens) == 0: #empty list\n",
    "            self.tokenize(document)\n",
    "        else:\n",
    "            spanishstemmer = SnowballStemmer('spanish')\n",
    "            self.stems = [spanishstemmer.stem(token) for token in self.tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows with empty 'content' field\n",
    "originData.dropna(subset = [\"content\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(stringObj, wordLong = 4):\n",
    "    '''\n",
    "    remove words with lenght <= wordLong\n",
    "    \n",
    "    return: list\n",
    "    '''\n",
    "    listObj = stringObj.split(' ')\n",
    "    words = list()\n",
    "    words = [word for word in listObj if len(word)>wordLong]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_less_frequent_words(myList, frequency=10):\n",
    "    from collections import Counter\n",
    "    words_freq = Counter(myList)\n",
    "    result = list()\n",
    "    words = [[elem[0]]*elem[1] for elem in words_freq.items() if elem[1] >= frequency]    \n",
    "    for val in words:\n",
    "        result.extend(val)\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "originData['content'] = originData['content'].map(lambda x: remove_short_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF = originData.copy(deep=True)\n",
    "myDF = myDF.drop(columns=['manually_assigned_tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDict = dict(zip(myDF.url, myDF.content)) # -> key: url, value:content (list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nonzero = 0\n",
    "vocab = set()\n",
    "for docterms in myDict.values():\n",
    "    unique_terms = set(docterms)    # all unique terms of this doc\n",
    "    vocab |= unique_terms           # set union: add unique terms of this doc\n",
    "    n_nonzero += len(unique_terms)  # add count of unique terms in this doc\n",
    "\n",
    "# make a list of document names\n",
    "# the order will be the same as in the dict\n",
    "docnames = list(myDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "docnames = np.array(docnames)\n",
    "vocab = np.array(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sorter = np.argsort(vocab)    # indices that sort \"vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensions of the matrix will be len(docnames) x len(vocab)\n",
    "ndocs = len(docnames)\n",
    "nvocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.empty(n_nonzero, dtype=np.intc)     # all non-zero term frequencies at data[k]\n",
    "rows = np.empty(n_nonzero, dtype=np.intc)     # row index for kth data item (kth term freq.)\n",
    "cols = np.empty(n_nonzero, dtype=np.intc)     # column index for kth data item (kth term freq.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0     # current index in the sparse matrix data\n",
    "# go through all documents with their terms\n",
    "for docname, terms in myDict.items():\n",
    "    # find indices into  such that, if the corresponding elements in  were\n",
    "    # inserted before the indices, the order of  would be preserved\n",
    "    # -> array of indices of  in \n",
    "    term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter=vocab_sorter)]\n",
    "\n",
    "    # count the unique terms of the document and get their vocabulary indices\n",
    "    uniq_indices, counts = np.unique(term_indices, return_counts=True)\n",
    "    n_vals = len(uniq_indices)  # = number of unique terms\n",
    "    ind_end = ind + n_vals  #  to  is the slice that we will fill with data\n",
    "\n",
    "    data[ind:ind_end] = counts                  # save the counts (term frequencies)\n",
    "    cols[ind:ind_end] = uniq_indices            # save the column index: index in \n",
    "    doc_idx = np.where(docnames == docname)     # get the document index for the document name\n",
    "    rows[ind:ind_end] = np.repeat(doc_idx, n_vals)  # save it as repeated value\n",
    "\n",
    "    ind = ind_end  # resume with next document -> add data to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "dtm = coo_matrix((data, (rows, cols)), shape=(ndocs, nvocab), dtype=np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = originData.groupby(['manually_assigned_tag'])['manually_assigned_tag'].nunique().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 63\n",
      "INFO:lda:vocab_size: 2120\n",
      "INFO:lda:n_words: 18673\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1000\n",
      "/home/sofia/.local/share/virtualenvs/topic_modeling-Gm6ev8G1/lib/python3.6/site-packages/lda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:lda:<0> log likelihood: -209409\n",
      "INFO:lda:<10> log likelihood: -112653\n",
      "INFO:lda:<20> log likelihood: -108564\n",
      "INFO:lda:<30> log likelihood: -106970\n",
      "INFO:lda:<40> log likelihood: -105691\n",
      "INFO:lda:<50> log likelihood: -105243\n",
      "INFO:lda:<60> log likelihood: -104789\n",
      "INFO:lda:<70> log likelihood: -104609\n",
      "INFO:lda:<80> log likelihood: -104280\n",
      "INFO:lda:<90> log likelihood: -103627\n",
      "INFO:lda:<100> log likelihood: -103060\n",
      "INFO:lda:<110> log likelihood: -103040\n",
      "INFO:lda:<120> log likelihood: -102659\n",
      "INFO:lda:<130> log likelihood: -102151\n",
      "INFO:lda:<140> log likelihood: -101763\n",
      "INFO:lda:<150> log likelihood: -101616\n",
      "INFO:lda:<160> log likelihood: -101051\n",
      "INFO:lda:<170> log likelihood: -101075\n",
      "INFO:lda:<180> log likelihood: -101180\n",
      "INFO:lda:<190> log likelihood: -101016\n",
      "INFO:lda:<200> log likelihood: -100999\n",
      "INFO:lda:<210> log likelihood: -100956\n",
      "INFO:lda:<220> log likelihood: -100901\n",
      "INFO:lda:<230> log likelihood: -100912\n",
      "INFO:lda:<240> log likelihood: -100847\n",
      "INFO:lda:<250> log likelihood: -101037\n",
      "INFO:lda:<260> log likelihood: -101091\n",
      "INFO:lda:<270> log likelihood: -100998\n",
      "INFO:lda:<280> log likelihood: -101070\n",
      "INFO:lda:<290> log likelihood: -100979\n",
      "INFO:lda:<300> log likelihood: -101030\n",
      "INFO:lda:<310> log likelihood: -100964\n",
      "INFO:lda:<320> log likelihood: -101148\n",
      "INFO:lda:<330> log likelihood: -101078\n",
      "INFO:lda:<340> log likelihood: -100880\n",
      "INFO:lda:<350> log likelihood: -100987\n",
      "INFO:lda:<360> log likelihood: -100809\n",
      "INFO:lda:<370> log likelihood: -101111\n",
      "INFO:lda:<380> log likelihood: -101039\n",
      "INFO:lda:<390> log likelihood: -101096\n",
      "INFO:lda:<400> log likelihood: -101010\n",
      "INFO:lda:<410> log likelihood: -101053\n",
      "INFO:lda:<420> log likelihood: -100758\n",
      "INFO:lda:<430> log likelihood: -100870\n",
      "INFO:lda:<440> log likelihood: -100881\n",
      "INFO:lda:<450> log likelihood: -100833\n",
      "INFO:lda:<460> log likelihood: -100792\n",
      "INFO:lda:<470> log likelihood: -100794\n",
      "INFO:lda:<480> log likelihood: -100906\n",
      "INFO:lda:<490> log likelihood: -101003\n",
      "INFO:lda:<500> log likelihood: -101076\n",
      "INFO:lda:<510> log likelihood: -100904\n",
      "INFO:lda:<520> log likelihood: -100956\n",
      "INFO:lda:<530> log likelihood: -100938\n",
      "INFO:lda:<540> log likelihood: -101045\n",
      "INFO:lda:<550> log likelihood: -100876\n",
      "INFO:lda:<560> log likelihood: -101043\n",
      "INFO:lda:<570> log likelihood: -100907\n",
      "INFO:lda:<580> log likelihood: -100860\n",
      "INFO:lda:<590> log likelihood: -101048\n",
      "INFO:lda:<600> log likelihood: -100894\n",
      "INFO:lda:<610> log likelihood: -100776\n",
      "INFO:lda:<620> log likelihood: -100821\n",
      "INFO:lda:<630> log likelihood: -100772\n",
      "INFO:lda:<640> log likelihood: -100953\n",
      "INFO:lda:<650> log likelihood: -100919\n",
      "INFO:lda:<660> log likelihood: -100845\n",
      "INFO:lda:<670> log likelihood: -100892\n",
      "INFO:lda:<680> log likelihood: -100812\n",
      "INFO:lda:<690> log likelihood: -100760\n",
      "INFO:lda:<700> log likelihood: -100975\n",
      "INFO:lda:<710> log likelihood: -100853\n",
      "INFO:lda:<720> log likelihood: -100898\n",
      "INFO:lda:<730> log likelihood: -100806\n",
      "INFO:lda:<740> log likelihood: -101004\n",
      "INFO:lda:<750> log likelihood: -101096\n",
      "INFO:lda:<760> log likelihood: -100973\n",
      "INFO:lda:<770> log likelihood: -100860\n",
      "INFO:lda:<780> log likelihood: -101002\n",
      "INFO:lda:<790> log likelihood: -100877\n",
      "INFO:lda:<800> log likelihood: -100992\n",
      "INFO:lda:<810> log likelihood: -100867\n",
      "INFO:lda:<820> log likelihood: -100912\n",
      "INFO:lda:<830> log likelihood: -100845\n",
      "INFO:lda:<840> log likelihood: -100781\n",
      "INFO:lda:<850> log likelihood: -100868\n",
      "INFO:lda:<860> log likelihood: -100798\n",
      "INFO:lda:<870> log likelihood: -100912\n",
      "INFO:lda:<880> log likelihood: -100870\n",
      "INFO:lda:<890> log likelihood: -101003\n",
      "INFO:lda:<900> log likelihood: -100882\n",
      "INFO:lda:<910> log likelihood: -100810\n",
      "INFO:lda:<920> log likelihood: -101065\n",
      "INFO:lda:<930> log likelihood: -100839\n",
      "INFO:lda:<940> log likelihood: -100819\n",
      "INFO:lda:<950> log likelihood: -100730\n",
      "INFO:lda:<960> log likelihood: -100718\n",
      "INFO:lda:<970> log likelihood: -100770\n",
      "INFO:lda:<980> log likelihood: -101077\n",
      "INFO:lda:<990> log likelihood: -100795\n",
      "INFO:lda:<999> log likelihood: -100902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: suplementos ciento libros presidente logos fuente universidad elena nuevo nicolas\n",
      "Topic 1: buenos aeropuerto deseos barcelona ciudad internacional destinos cuadras aeropuertos nueva\n",
      "Topic 2: banco beneficios exterior financiacion canales internacionalizacion personas cuenta productos acerca\n",
      "Topic 3: servicios sucesos internacionales escenarios nacionales actualidad tabla deportes quedo muerte\n",
      "Topic 4: mercadolibre report close contact respond cancel espera videos reported design\n",
      "Topic 5: cordoba florida places puerto barcelona mexico london carnes carlos social\n",
      "Topic 6: elgatoylacaja video edcep edcio cambio nueva mexico libros unidos escuela\n",
      "Topic 7: deshacer gracias cuenta cerrar respuestas informacion puede enlace responder cualquier\n",
      "Topic 8: internacional nuevo basicas esencial tambien despues precios hacer nicolas celebridades\n",
      "Topic 9: macro conocenos canales cparentrq financiaciones propuestas exportacion sueldos equipment servicios\n",
      "Topic 10: servicios productos cuotas promociones conoce frecuentes beneficios descarga nuevo hacer\n",
      "Topic 11: cadena cordoba conductores deportes sociedad colectivo jubilaciones espectaculos redacci sigue\n",
      "Topic 12: desayuno especial precio cacao compuesta libreria libre fideos capsulas doble\n",
      "Topic 13: electrico videojuegos precio consigue trendencias nuevo seleccion djuegos desarrolladores apuesta\n",
      "Topic 14: ciudadanos clasificados videos redaccion video espacio retenciones sucesos publicidad actualidad\n",
      "Topic 15: precio electro congelados frescos libre aespa cocina comparaciones congeladas cuidado\n",
      "Topic 16: beneficios cuenta productos cuentas personas soluciones informacion canales financiacion nueva\n",
      "Topic 17: despegar propiedades barrio propiedad departamento partido cordoba departamentos construccion localidad\n",
      "Topic 18: deportes sociedad hombre buenos ciudad retenciones contacto infectado libertadores paciente\n",
      "Topic 19: alvearsupermercados claro contenido almacen personas lacteos productos video quesos descuento\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "model = lda.LDA(n_topics=tags, n_iter=1000, random_state=1)\n",
    "model.fit(dtm)\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/processed/processed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = data['content'].map(lambda x: remove_less_frequent_words(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/processed/processed_dataset_less_frequent_words_removed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
